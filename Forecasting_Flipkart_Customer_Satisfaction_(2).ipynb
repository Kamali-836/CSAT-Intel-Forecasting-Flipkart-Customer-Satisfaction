{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vncDsAP0Gaoa"
      },
      "source": [
        "# **CSAT Intel: Forecasting Flipkart Customer Satisfaction**    -\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beRrZCGUAJYm"
      },
      "source": [
        "##### **Project Type**    : Classification\n",
        "##### **Contribution**    : Individual\n",
        "##### **- R.KAMALI**      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJNUwmbgGyua"
      },
      "source": [
        "# **Project Summary**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6v_1wHtG2nS"
      },
      "source": [
        "Customer satisfaction is a critical performance metric in the fast-paced world of e-commerce today. Knowing how users feel after interacting with support is essential for a top platform like Flipkart in order to retain users and increase operational efficiency. Direct information about customer sentiment can be obtained from the **CSAT (Customer Satisfaction) Score**, which is usually obtained through feedback surveys and ranges from 1 to 5. But not all clients complete the survey, and waiting for answers causes delays. This project uses machine learning to try to close that gap.\n",
        "\n",
        "\"**CSAT Intel**\" is a predictive analytics tool that uses classification models to forecast customer satisfaction ratings from support interaction data. Building a system that can reliably categorize CSAT scores (1–5) based on a variety of operational characteristics, including issue type, agent profile, shift timing, and interaction response time.\n",
        "\n",
        "\n",
        "#### **KEY COLUMNS:**\n",
        "The dataset used contains over **85,000** historical support records.\n",
        "\n",
        "**Unique id** – Unique identifier for each support ticket or customer interaction.\n",
        "\n",
        "**channel_name** – Type of interaction channel (e.g., Inbound, Outcall, Self-service).\n",
        "\n",
        "**category** – High-level classification of the issue (e.g., Product Queries, Returns, Order Related).\n",
        "\n",
        "**Sub-category** – More specific sub-type of issue within the main category (e.g., Life Insurance, Installation/demo).\n",
        "\n",
        "**Customer Remarks** – Text remarks or complaints provided by the customer.\n",
        "\n",
        "**Order_id** – Unique Flipkart order ID associated with the support interaction.\n",
        "\n",
        "**order_date_time** – Date and time when the customer placed the order.\n",
        "\n",
        "**Issue_reported at** – Timestamp indicating when the issue was first raised.\n",
        "\n",
        "**issue_responded** – Timestamp indicating when the agent responded to the issue.\n",
        "\n",
        "**Survey_response_Date** – Date when the customer completed the CSAT feedback survey.\n",
        "\n",
        "**Customer_City** – City from which the customer contacted Flipkart support.\n",
        "\n",
        "**Product_category**– The category of the product involved in the support ticket.\n",
        "\n",
        "**Item_price** – Price of the item mentioned in the order (in INR).\n",
        "\n",
        "**connected_handling_time** – Total connected time (in minutes) the agent spent handling the issue.\n",
        "\n",
        "**Agent_name** – Name of the customer service agent handling the ticket.\n",
        "\n",
        "**Supervisor** – Supervisor who oversees the performance of the agent.\n",
        "\n",
        "**Manager** – Manager responsible for the agent’s overall performance.\n",
        "\n",
        "**Tenure Bucket** – Experience range of the agent (e.g., 0–30 days, 31–90 days, >90 days).\n",
        "\n",
        "**Agent Shift** – Shift timing when the agent handled the ticket (Morning, Evening, Night).\n",
        "\n",
        "**CSAT Score** – Customer Satisfaction Score, ranging from 1 (Very Dissatisfied) to 5 (Very Satisfied) — [ target variable for prediction ]\n",
        "\n",
        "#### **CLASSIFICATION MODELS USED:**\n",
        "1.Random Forest Classifier :\n",
        "\n",
        "Robust, easy to tune, works well with categorical and numerical data, and handles imbalanced classes better than many others.\n",
        "Low risk of overfitting, good performance without much parameter tuning.\n",
        "\n",
        "2️. Logistic Regression :\n",
        "\n",
        "Simple, interpretable, and fast. Great for establishing a baseline performance.\n",
        "Helps understand the impact of individual features on prediction. Useful for explaining model predictions to non-technical stakeholders.\n",
        "\n",
        "#### **EVALUATION METRICS:**\n",
        "**Accuracy** – Measures how often the model predicts correctly overall.\n",
        "\n",
        "**Precision, Recall, F1-Score** – Precision shows correct positives, Recall shows missed positives, and F1 balances both.\n",
        "\n",
        "**Confusion Matrix** – Shows actual vs. predicted values to understand model errors.\n",
        "\n",
        "**Cross-Validation** – Tests model reliability by training on multiple data splits.\n",
        "\n",
        "**Hyperparameter Tuning (GridSearchCV)**– Finds the best model settings for improved performance\n",
        "\n",
        "#### **TARGET VARIABLE:**\n",
        "The CSAT values were optionally divided into three segments to help make the insights more actionable:\n",
        "\n",
        "Low (1–2): Customers who are at risk\n",
        "\n",
        "Neutral (3): Moderately dangerous\n",
        "\n",
        "High (4–5): Contented clients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6K7xa23Elo4"
      },
      "source": [
        "# **GitHub Link:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1o69JH3Eqqn"
      },
      "source": [
        "https://github.com/Kamali-836/CSAT-Intel-Forecasting-Flipkart-Customer-Satisfaction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQaldy8SH6Dl"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpeJGUA3kjGy"
      },
      "source": [
        "To predict Customer Satisfaction (CSAT) scores using Flipkart’s customer support data by analyzing operational features such as issue category, agent profile, and response time, with the goal of identifying dissatisfied customers early and enabling timely service improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDgbUHAGgjLW"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_i_v8NEhb9l"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhfV-JJviCcP"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3lxredqlCYt"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Vqi-pPk-HR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3d0fd6f6-034a-4460-c91f-7c366736fea8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ca8eea0-86ea-438b-aef5-baa0ae46bc3b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ca8eea0-86ea-438b-aef5-baa0ae46bc3b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import pandas as pd                 # For working with dataframes\n",
        "import numpy as np                  # For numerical operations\n",
        "\n",
        "import matplotlib.pyplot as plt     # For basic plotting\n",
        "import seaborn as sns               # For statistical visualizations\n",
        "%matplotlib inline\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")   # Suppress warnings for cleaner output\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV  # For splitting and model validation\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler                      # For encoding and scaling\n",
        "from sklearn.pipeline import Pipeline                                                # For combining preprocessing + model\n",
        "from sklearn.compose import ColumnTransformer                                        # For column-wise preprocessing\n",
        "from scipy import stats                                                              # For hypothesis testing\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.decomposition import PCA                                                # Dimensionality Reduction\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression              # Baseline model\n",
        "from sklearn.ensemble import RandomForestClassifier              # Tree-based model\n",
        "from xgboost import XGBClassifier                                # Boosted tree model\n",
        "\n",
        "import joblib\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix  # For measuring model performance\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RnN4peoiCZX"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    df = pd.read_csv(\"Customer_support_data.csv\")\n",
        "    print(\"Dataset loaded !\")\n",
        "except FileNotFoundError:\n",
        "    print(\"File not found. Please check the file name or path.\")\n",
        "except Exception as e:\n",
        "    print(\"An unexpected error occurred:\", str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x71ZqKXriCWQ"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "outputs": [],
      "source": [
        "# first 5 rows of the dataset\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hBIi_osiCS2"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "outputs": [],
      "source": [
        "# Total number of rows and columns in the dataset\n",
        "\n",
        "print(\"Number of Rows:\", df.shape[0])\n",
        "print(\"Number of Columns:\", df.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHwYmJAmNHm"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "outputs": [],
      "source": [
        "# Display data types, non-null counts, and memory usage\n",
        "\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35m5QtbWiB9F"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "outputs": [],
      "source": [
        "# Duplicate rows in the dataset\n",
        "\n",
        "dcount = df.duplicated().sum()\n",
        "print(\"Number of duplicate rows:\", dcount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoPl-ycgm1ru"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "outputs": [],
      "source": [
        "# Count of missing/null values in each column\n",
        "\n",
        "miss = df.isnull().sum()\n",
        "miss = miss[miss > 0].sort_values()\n",
        "\n",
        "# Display the columns with missing values\n",
        "\n",
        "print(\"Number of columns with missing values:\", len(miss))\n",
        "print(\"\\nColumns with missing values:\\n\")\n",
        "print(miss)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "outputs": [],
      "source": [
        "# Bar chart showing count of missing values per column\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "miss.plot(kind='barh', color='coral')\n",
        "plt.title(\"Missing Values Count per Column\")\n",
        "plt.xlabel(\"Number of Missing Values\")\n",
        "plt.ylabel(\"Columns\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0kj-8xxnORC"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoNAAC-nUe_"
      },
      "source": [
        "The dataset includes 85,907 rows and 20 columns, without any duplicate records. The target variable is CSAT Score, which ranges from 1 to 5, and it has no missing values. Seven columns contain missing values; for example, connected_handling_time is missing in more than 99% of the rows. Most features are categorical. Columns like Issue_reported and issue_responded can help determine response time. The dataset is suitable for classification, but it needs handling of missing values, encoding, and feature engineering before modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "\n",
        "df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "outputs": [],
      "source": [
        "# Dataset Describe\n",
        "\n",
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3PMJOP6ngxN"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "outputs": [],
      "source": [
        "# Display the number of unique values in each column\n",
        "\n",
        "ucount = df.nunique().sort_values()\n",
        "print(\"Number of unique values per column:\\n\")\n",
        "print(ucount)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dauF4eBmngu3"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKJF3rekwFvQ"
      },
      "source": [
        "### Data Wrangling Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "# 1. Drop highly sparse columns\n",
        "\n",
        "df = df.drop(columns=[\n",
        "    'connected_handling_time',  # 99.7% missing\n",
        "    'Customer Remarks',         # 66% missing\n",
        "    'order_date_time'           # 80% missing\n",
        "])\n",
        "\n",
        "\n",
        "# 2. Drop rows with missing values in important features\n",
        "\n",
        "df = df.dropna(subset = [\n",
        "    'Order_id', 'Product_category', 'Item_price',\n",
        "    'Customer_City'\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Handling Outliers & Outlier Treatments\n",
        "\n",
        "# 1. Convert date columns to datetime format\n",
        "\n",
        "df['Issue_reported at'] = pd.to_datetime(df['Issue_reported at'], errors='coerce')\n",
        "df['issue_responded'] = pd.to_datetime(df['issue_responded'], errors='coerce')\n",
        "\n",
        "\n",
        "# 2. Create a new feature — response time in minutes\n",
        "\n",
        "df['response_time_mins'] = (df['issue_responded'] - df['Issue_reported at']).dt.total_seconds() / 60\n",
        "\n",
        "\n",
        "# 3. Drop rows with invalid or negative response times\n",
        "\n",
        "df = df[df['response_time_mins'] >= 0]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Reset index after cleaning\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "print(\"Data is cleaned and ready for analysis.\")\n",
        "print(\"New shape:\", df.shape)\n",
        "miss = df.isnull().sum()\n",
        "print(miss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSa1f5Uengrz"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbyXE7I1olp8"
      },
      "source": [
        "**Manipulations Done:**\n",
        "\n",
        "1. Removed connected_handling_time, Customer Remarks, and order_date_time due to extremely high missing values (>65%–99%).\n",
        "\n",
        "2. Removed records with nulls in Order_id, Product_category, Item_price, and Customer_City.\n",
        "\n",
        "3. Converted date columns to datetime format. Parsed Issue_reported at and issue_responded to enable time-based analysis.\n",
        "\n",
        "4. A new feature created, response_time_mins = time taken (in minutes) for the agent to respond to the issue.\n",
        "\n",
        "5. Filtered out rows where response time was negative or invalid.\n",
        "\n",
        "6. Reindexed the cleaned dataset for consistency.\n",
        "\n",
        "**Insights Found:**\n",
        "\n",
        "1. Several columns were not usable due to excessive missing data, confirming the need for targeted data collection.\n",
        "\n",
        "2. Response time is now a measurable metric, which can be analyzed against CSAT scores.\n",
        "\n",
        "3. The dataset after cleaning is now free of nulls in critical fields, enabling smooth EDA and modeling.\n",
        "\n",
        "4. Many columns are categorical, and will require encoding before modeling.\n",
        "\n",
        "5. The dataset is now in a structured, analysis-ready state with consistent formatting and well-defined features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GF8Ens_Soomf"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wOQAZs5pc--"
      },
      "source": [
        "#### Chart - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "outputs": [],
      "source": [
        "# UNIVARIATE ANALYSIS\n",
        "\n",
        "# CSAT Score Distribution\n",
        "\n",
        "sns.countplot(x='CSAT Score', data=df, palette='pastel')\n",
        "plt.title(\"CSAT Score Distribution\")\n",
        "plt.xlabel(\"CSAT Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5QZ13OEpz2H"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XESiWehPqBRc"
      },
      "source": [
        "A countplot is great for visualizing the frequency of each CSAT score so that we can see the class imbalance within the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_j1G7yiqdRP"
      },
      "source": [
        "Most customers used a score of 5 (very satisfied) while there were few occurrences of scores 1-3, and hence, a strong sign of class imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "448CDAPjqfQr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cspy4FjqxJW"
      },
      "source": [
        "Yes, this helps us focus more on the customers who gave low scores. If we don’t fix the imbalance, the model may always guess score 5, which can miss unhappy customers — and that can hurt the business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSlN3yHqYklG"
      },
      "source": [
        "#### Chart - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "outputs": [],
      "source": [
        "# UNIVARIATE ANALYSIS\n",
        "\n",
        "# Issue Category Distribution\n",
        "\n",
        "sns.countplot(y='category', data=df, order=df['category'].value_counts().index, palette='Set2')\n",
        "plt.title(\"Support Issue Categories\")\n",
        "plt.xlabel(\"Count\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6dVpIINYklI"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aaW0BYyYklI"
      },
      "source": [
        "A horizontal countplot is perfect to show how many tickets fall under each issue category, especially when the labels are long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijmpgYnKYklI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSx9atu2YklI"
      },
      "source": [
        "Most support issues are about Returns and Order Related problems. Other categories like Refunds and Product Queries are much less frequent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JiQyfWJYklI"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcBbebzrYklV"
      },
      "source": [
        "Yes, knowing that Returns and Order issues are the most common helps Flipkart focus its resources, improve those processes, and train agents better.\n",
        "If these high-volume categories are not handled well, it can lead to more low CSAT scores and negative business impact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM7whBJCYoAo"
      },
      "source": [
        "#### Chart - 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "outputs": [],
      "source": [
        "# UNIVARIATE ANALYSIS\n",
        "\n",
        "# Response Time Distribution\n",
        "\n",
        "sns.histplot(df['response_time_mins'], bins=50, kde=True, color='skyblue')\n",
        "plt.title(\"Distribution of Agent Response Time (minutes)\")\n",
        "plt.xlabel(\"Response Time (minutes)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fge-S5ZAYoAp"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dBItgRVYoAp"
      },
      "source": [
        "A histogram with KDE helps us understand how response times are spread out and whether there are outliers or long delays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85gYPyotYoAp"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jstXR6OYoAp"
      },
      "source": [
        "Most tickets were responded to quickly (under 10,000 minutes), but a few took extremely long — which causes a skewed distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoGjAbkUYoAp"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      },
      "source": [
        "Yes. The long response time outliers highlight inefficiencies. Fixing these can improve CSAT scores. If ignored, they may hurt customer trust and satisfaction (negative impact)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVIhbpwhxJp3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Of9eVA-YrdM"
      },
      "source": [
        "#### Chart - 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "outputs": [],
      "source": [
        "# BIVARIATE ANALYSIS\n",
        "\n",
        "# CSAT Score vs Channel Name (Categorical + Categorical)\n",
        "\n",
        "sns.countplot(x='channel_name', hue='CSAT Score', data=df, palette='Set2')\n",
        "plt.title(\"CSAT Score by Support Channel\")\n",
        "plt.xlabel(\"Support Channel\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iky9q4vBYrdO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJRCwT6DYrdO"
      },
      "source": [
        "A grouped countplot with hue='CSAT Score' helps compare CSAT levels across different support channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6T5p64dYrdO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      },
      "source": [
        "The Inbound channel has the highest number of responses, both good and bad. Most 5-star scores also come from Inbound, while Outcall and Email channels receive fewer ratings overall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-Ehk30pYrdP"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNxxz7MYrdP"
      },
      "source": [
        "Yes — this tells Flipkart that improving the Inbound channel can impact CSAT the most. However, neglecting Email or Outcall channels may leave some customers unsatisfied, risking negative growth from overlooked segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bamQiAODYuh1"
      },
      "source": [
        "#### Chart - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "outputs": [],
      "source": [
        "# BIVARIATE ANALYSIS\n",
        "\n",
        "# Item Price vs Response Time (Numerical + Numerical)\n",
        "\n",
        "sns.scatterplot(x='Item_price', y='response_time_mins', data=df, hue='CSAT Score', palette='tab10')\n",
        "plt.title(\"Item Price vs Response Time (colored by CSAT Score)\")\n",
        "plt.xlabel(\"Item Price (INR)\")\n",
        "plt.ylabel(\"Response Time (minutes)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcxuIMRPYuh3"
      },
      "source": [
        "A scatterplot is the best way to observe the relationship between two numeric variables — in this case, price and response time — while also showing CSAT scores using color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwzvFGzlYuh3"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyqkiB8YYuh3"
      },
      "source": [
        "There is no strong visible correlation between item price and response time. Most values are clustered near zero response time, but some outliers exist with very long wait times — even for expensive items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYpmQ266Yuh3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      },
      "source": [
        "Yes — this helps identify that long delays are not tied to price, meaning all orders need equal attention. Ignoring these high-delay outliers, especially for premium orders, could reduce satisfaction and damage trust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OH-pJp9IphqM"
      },
      "source": [
        "#### Chart - 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "outputs": [],
      "source": [
        "# MULTIVARIATE ANALYSIS\n",
        "\n",
        "# Avg Response Time by Category and Shift\n",
        "\n",
        "pivot = df.pivot_table(values='response_time_mins', index='category', columns='Agent Shift', aggfunc='mean')\n",
        "sns.heatmap(pivot, annot=True, fmt=\".1f\", cmap=\"YlGnBu\")\n",
        "plt.title(\"Avg Response Time by Category & Shift\")\n",
        "plt.xlabel(\"Agent Shift\")\n",
        "plt.ylabel(\"Issue Category\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbFf2-_FphqN"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loh7H2nzphqN"
      },
      "source": [
        "A heatmap with a pivot table helps visualize the average response time across two dimensions : issue categories and agent shifts , making it a true multivariate view."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ouA3fa0phqN"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VECbqPI7phqN"
      },
      "source": [
        "Some categories like \"Product Queries and Payments\" show extremely high response times during specific shifts , while others are handled faster in shifts like Morning or Split. “Offers & Cashback” at Night also stands out with a huge delay."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Seke61FWphqN"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW4_bGpfphqN"
      },
      "source": [
        "Yes — identifying which categories and shifts have slower response times helps Flipkart optimize shift scheduling and staffing. If not corrected, these delays can lower customer satisfaction and lead to negative reviews or churn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZR9WyysphqO"
      },
      "source": [
        "#### Chart - 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "outputs": [],
      "source": [
        "# MULTIVARIATE ANALYSIS\n",
        "\n",
        "# CSAT by Agent Shift and Tenure Bucket\n",
        "\n",
        "pivot = df.pivot_table(index='Agent Shift', columns='Tenure Bucket', values='CSAT Score', aggfunc='count')\n",
        "pivot.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='tab20')\n",
        "plt.title(\"CSAT Count by Agent Shift and Tenure Bucket\")\n",
        "plt.xlabel(\"Agent Shift\")\n",
        "plt.ylabel(\"Number of Tickets\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7wYXLtphqO"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob8u6rCTphqO"
      },
      "source": [
        "A stacked bar chart helps compare how many CSAT scores came from different agent experience levels across all shifts , showing 3 variables at once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrbJ2SmphqO"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZtgC_hjphqO"
      },
      "source": [
        "The Morning and Evening shifts have the highest number of tickets. Most are handled by agents with >90 days experience, but there’s also a strong presence of less experienced agents , especially in Morning shifts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFu4xreNphqO"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey_0qi68phqO"
      },
      "source": [
        "Yes — it shows which shifts are busiest and which tenure levels are carrying the load. If new agents are overburdened or not trained enough during high-traffic shifts, it can lead to errors and lower CSAT, which may hurt customer trust."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### Chart - 8 - Correlation Heatmap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "outputs": [],
      "source": [
        "# Correlation Matrix\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "plt.title(\"Correlation Matrix of Numeric Features\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "A heatmap of correlation values is the clearest way to show how strongly numerical features are related to each other (positive or negative)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "All correlations are weak. CSAT Score has a slight negative correlation with both item price and response time, meaning longer delays or higher prices may reduce satisfaction — but the effect is minimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### Chart - 9 - Pair Plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "outputs": [],
      "source": [
        "# Pair Plot – Numeric Feature Relationships\n",
        "sns.pairplot(df[['CSAT Score', 'Item_price', 'response_time_mins']], hue='CSAT Score', palette='Set2')\n",
        "plt.suptitle(\"Pair Plot of Key Numeric Features\", y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "A pair plot is great for exploring relationships between multiple numeric features at once, while using color (hue) to show CSAT Score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "There’s no clear linear relationship between CSAT and either price or response time, but most data is concentrated at lower values. It also shows how different CSAT scores are spread across price and time ranges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "*Average response time is the same across all agent shifts.*\n",
        "\n",
        "**Null Hypothesis (H₀)**: There is no difference in the average agent response time across different agent shifts.\n",
        "\n",
        "**Alternate Hypothesis (H₁)**: At least one agent shift has a significantly different average response time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "outputs": [],
      "source": [
        "# Grouping response times by agent shift\n",
        "\n",
        "groups = [\n",
        "    df[df['Agent Shift'] == shift]['response_time_mins'].dropna()\n",
        "    for shift in df['Agent Shift'].unique()\n",
        "]\n",
        "\n",
        "# Perform One-way ANOVA\n",
        "\n",
        "f_stat, p_val = stats.f_oneway(*groups)\n",
        "print(f\"F-Statistic: {f_stat:.2f}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "One-way ANOVA (Analysis of Variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "One-way ANOVA is appropriate because:\n",
        "\n",
        "1.   The dependent variable (response_time_mins) is numerical.\n",
        "2.   The independent variable (Agent Shift) is categorical with more than 2 groups.\n",
        "3.   We are testing if the means differ across those groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "*The distribution of CSAT scores is independent of the issue category.*\n",
        "\n",
        "**Null Hypothesis (H₀)**: CSAT scores are independent of the issue category.\n",
        "\n",
        "**Alternate Hypothesis (H₁)**: CSAT scores are associated with the issue category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "outputs": [],
      "source": [
        "# Create a contingency table (Issue Category × CSAT Score)\n",
        "\n",
        "ctable = pd.crosstab(df['category'], df['CSAT Score'])\n",
        "\n",
        "# Perform Chi-square test of independence\n",
        "\n",
        "chi2_stat, p_val, dof, expected = stats.chi2_contingency(ctable)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2_stat:.2f}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "Chi-Square Test of Independence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "Because both variables : category and CSAT Score are categorical, and we are checking whether there is an association between them. The Chi-square test is designed exactly for this kind of comparison."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "*There is no correlation between response time and CSAT score.*\n",
        "\n",
        "**Null Hypothesis (H₀)**: There is no monotonic correlation between response time (response_time_mins) and CSAT score.\n",
        "\n",
        "**Alternate Hypothesis (H₁)**: There is a monotonic correlation between response time and CSAT score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "outputs": [],
      "source": [
        "# Drop missing values\n",
        "\n",
        "subset = df[['response_time_mins', 'CSAT Score']].dropna()\n",
        "\n",
        "# Spearman Rank Correlation\n",
        "\n",
        "corr, p_val = spearmanr(subset['response_time_mins'], subset['CSAT Score'])\n",
        "\n",
        "print(f\"Spearman Correlation: {corr:.2f}\")\n",
        "print(f\"P-Value: {p_val:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "Spearman Rank Correlation Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "CSAT Score is an ordinal variable (1 to 5). Response time is numeric, but skewed with many outliers. We are interested in checking if there's a monotonic (not necessarily linear) relationship. Spearman is best suited for non-linear, non-normal data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 1. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select categorical features for encoding\n",
        "categorical_features = ['channel_name', 'category', 'Sub-category', 'Agent Shift', 'Tenure Bucket', 'Product_category', 'Customer_City']\n",
        "categorical_features = [col for col in categorical_features if col in df.columns]\n",
        "\n",
        "# Select numerical features\n",
        "numerical_features = ['Item_price', 'response_time_mins']\n",
        "numerical_features = [col for col in numerical_features if col in df.columns]\n",
        "\n",
        "# Encode target variable CSAT Score into 3 categories for better handling of imbalance\n",
        "def categorize_csat(score):\n",
        "    if score <= 2:\n",
        "        return 'Low'\n",
        "    elif score == 3:\n",
        "         return 'Neutral'\n",
        "    else:\n",
        "        return 'High'\n",
        "df['CSAT_Category'] = df['CSAT Score'].apply(categorize_csat)\n",
        "\n",
        "# Prepare features and target\n",
        "X = df[categorical_features + numerical_features]\n",
        "y = df['CSAT_Category']\n",
        "\n",
        "print(\"Categorical features for encoding:\", categorical_features)\n",
        "print(\"Numerical features for scaling:\", numerical_features)\n",
        "print(\"Target variable categories:\", y.unique())"
      ],
      "metadata": {
        "id": "9IWWDdUfWt81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I chose OneHotEncoder because:\n",
        "\n",
        "It handles nominal categorical features without implying any order.\n",
        "\n",
        "It ensures machine learning models interpret categories correctly as separate entities.\n",
        "\n",
        "It avoids data distortion and works well with algorithms like Logistic Regression, Random Forest, and XGBoost."
      ],
      "metadata": {
        "id": "uJIrqkUs-HHT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 2. Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Log transformation for response_time_mins to handle skewness\n",
        "if 'response_time_mins' in X.columns:\n",
        "    X['response_time_mins'] = np.log1p(X['response_time_mins'])\n",
        "    print(\"Applied log transformation to response_time_mins to reduce skewness.\")"
      ],
      "metadata": {
        "id": "3U3j18NVW4br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data needed transformation due to skewness in the response_time_mins feature. I have used log transformation (np.log1p) to reduce skewness and normalize the distribution. This helps improve model performance by making the data more symmetric and less sensitive to extreme values."
      ],
      "metadata": {
        "id": "cmiNqllp-y6U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 3. Data Scaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Apply preprocessing\n",
        "X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "print(\"Data preprocessing completed. Shape after preprocessing:\", X_processed.shape)"
      ],
      "metadata": {
        "id": "Uk22LstkW_f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 4. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply PCA to reduce dimensionality\n",
        "pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
        "X_pca = pca.fit_transform(X_processed)\n",
        "\n",
        "print(f\"PCA reduced dimensions from {X_processed.shape[1]} to {X_pca.shape[1]} components\")\n",
        "print(f\"Explained variance ratio: {sum(pca.explained_variance_ratio_):.2f}\")"
      ],
      "metadata": {
        "id": "ZcOtUEV5XEBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, dimensionality reduction was needed to reduce the high number of features after encoding, which can lead to overfitting and increased computation time.\n"
      ],
      "metadata": {
        "id": "dZ-zDwFV_MP8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used PCA (Principal Component Analysis) to retain 95% of the variance while reducing dimensions.\n",
        "PCA was chosen because it removes redundant features, improves model efficiency, and helps in handling multicollinearity."
      ],
      "metadata": {
        "id": "sgQmzcBi_SsM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 5. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_pca, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(\"Data splitting completed:\")\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Testing set shape: {X_test.shape}\")\n",
        "print(f\"Class distribution in training set:\\n{y_train.value_counts(normalize=True)}\")\n",
        "print(f\"Class distribution in testing set:\\n{y_test.value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "yFy6BQV7XH6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used a 80:20 train-test split ratio, meaning 80% of the data was used for training and 20% for testing.\n",
        "This is a standard and balanced choice that ensures the model has enough data to learn patterns while reserving sufficient data to evaluate its performance.\n",
        "Also used stratification to maintain the class distribution across both sets, which is important for imbalanced datasets like CSAT categories."
      ],
      "metadata": {
        "id": "NONy7VOW_qqa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 6. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to handle class imbalance\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"After SMOTE resampling:\")\n",
        "print(f\"Training set shape: {X_train_resampled.shape}\")\n",
        "print(f\"Class distribution after resampling:\\n{y_train_resampled.value_counts(normalize=True)}\")"
      ],
      "metadata": {
        "id": "TwgLWS0jXMPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset is imbalanced because some CSAT categories (like 'Low' or 'Neutral') occur much less frequently than others.\n",
        "An imbalanced dataset can cause the model to be biased toward the majority class and perform poorly on minority classes."
      ],
      "metadata": {
        "id": "XSNWiVa1AIMq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To handle this, SMOTE (Synthetic Minority Over-sampling Technique) is used, which generates synthetic samples for minority classes to balance the dataset and improve model performance."
      ],
      "metadata": {
        "id": "DaCWUqzTALMq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize and train Logistic Regression model\n",
        "\n",
        "log_reg = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')\n",
        "log_reg.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "# Cross-validation\n",
        "\n",
        "cv_scores = cross_val_score(log_reg, X_train_resampled, y_train_resampled, cv=5, scoring='f1_weighted')\n",
        "print(f\"Logistic Regression Cross-Validation F1 Score: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    LogisticRegression(random_state=42, class_weight='balanced'),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train_resampled, y_train_resampled)\n",
        "best_log_reg = grid_search.best_estimator_\n",
        "print(f\"Best Logistic Regression parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best F1 Score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_pred = best_log_reg.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Logistic Regression Test Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Low', 'Neutral', 'High'], yticklabels=['Low', 'Neutral', 'High'])\n",
        "plt.title('Confusion Matrix - Logistic Regression')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q9DSP8ycXXZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"### ML Model - 2: Random Forest Classifier (Optimized for Speed)\"\"\"\n",
        "# Initialize with fewer trees for faster training\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=100,  # Reduced from default 100\n",
        "    random_state=42,\n",
        "    class_weight='balanced',\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf.fit(X_train_resampled, y_train_resampled)"
      ],
      "metadata": {
        "id": "J7DlCSCtnTou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation with fewer folds\n",
        "cv_scores = cross_val_score(rf, X_train_resampled, y_train_resampled, cv=3, scoring='f1_weighted')\n",
        "print(f\"Random Forest Cross-Validation F1 Score: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")"
      ],
      "metadata": {
        "id": "P0Rh1_ginat6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning with RandomizedSearchCV (much faster)\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 150],  # Reduced options\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2],\n",
        "    'max_features': ['sqrt']\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    rf,\n",
        "    param_dist,\n",
        "    n_iter=10,  # Test only 10 random combinations\n",
        "    cv=3,       # 3-fold instead of 5-fold\n",
        "    scoring='f1_weighted',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_resampled, y_train_resampled)\n",
        "best_rf = random_search.best_estimator_\n",
        "print(f\"Best Random Forest parameters: {random_search.best_params_}\")\n",
        "print(f\"Best F1 Score: {random_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "bwsMvmcbnfmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "y_pred = best_rf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Random Forest Test Performance:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "joblib.dump(best_rf, 'random_forest_model.joblib')\n",
        "print(\"✅ random_forest_model.joblib created successfully!\")\n",
        "files.download('random_forest_model.joblib')"
      ],
      "metadata": {
        "id": "V-9OjiV3nlOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['Low', 'Neutral', 'High'],\n",
        "            yticklabels=['Low', 'Neutral', 'High'])\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vMlLs_jCYJiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "Used accuracy, weighted precision, recall, F1 score, and the confusion matrix to evaluate model performance.\n",
        "Precision helps reduce costly false positives, while recall minimizes missed important cases (false negatives).\n",
        "F1 score balances precision and recall, which is crucial for imbalanced, multi-class problems.\n",
        "This approach aligns with business needs by accurately capturing risks across all classes, especially critical ones like “High.”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "I picked the Random Forest model because it works better with complex data than Logistic Regression.\n",
        "It gave higher F1 scores during testing, meaning it made more accurate predictions overall.\n",
        "Random Forest uses many decision trees which helps avoid mistakes from overfitting.\n",
        "It also handles imbalanced classes well, so it treats all groups fairly.\n",
        "Because of this, it’s the best choice for making reliable business decisions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "Random Forest is a group of decision trees that work together to make better predictions.\n",
        "Each tree looks at different parts of the data, and the final decision is based on the majority vote.\n",
        "This helps the model handle complex patterns and reduce errors.\n",
        "We can see which features are most important by checking the model’s feature importance scores.\n",
        "Tools like SHAP can also explain how each feature affects the model’s predictions in detail."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# LABMENTIX/\n",
        "# ├── app.py\n",
        "# ├── random_forest_model.joblib\n",
        "# └── svd_transformer.joblib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from IPython.display import Image\n",
        "\n",
        "url = 'https://github.com/Kamali-836/CSAT-Intel-Forecasting-Flipkart-Customer-Satisfaction/blob/main/Screenshot%202025-07-11%20015304.png'\n",
        "\n",
        "img_data = requests.get(url).content\n",
        "with open('your_image.png', 'wb') as handler:\n",
        "    handler.write(img_data)\n",
        "\n",
        "Image('Screenshot 2025-07-11 015304.png')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "This project predicts CSAT scores using machine learning on Flipkart support data. It helps identify customer satisfaction levels even when survey responses are missing. Important features like issue category, agent shift, and response time were used. OneHotEncoding, log transformation, and PCA were applied for effective preprocessing. SMOTE was used to handle class imbalance and improve prediction accuracy. Models like Random Forest and Logistic Regression were trained and tuned. Evaluation metrics like Accuracy, F1-Score, and Confusion Matrix ensured reliable results. Here the finalized model is **RANDOM FOREST** with accuracy approx. **0.72**. The CSAT score was categorized into Low, Neutral, and High for actionable insights. The model helps Flipkart prioritize customer support and retain unhappy users. Overall, the system boosts customer satisfaction tracking and service quality at scale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIfDvo9L0UH2"
      },
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "cell_execution_strategy": "setup",
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "mDgbUHAGgjLW",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "KSlN3yHqYklG",
        "EM7whBJCYoAo",
        "4Of9eVA-YrdM",
        "bamQiAODYuh1",
        "OH-pJp9IphqM",
        "BZR9WyysphqO",
        "rFu4xreNphqO",
        "NC_X3p0fY2L0",
        "q29F0dvdveiT",
        "g-ATYxFrGrvw",
        "8yEUt7NnHlrM",
        "4_0_7-oCpUZd",
        "yLjJCtPM0KBk",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "BhH2vgX9EjGr",
        "P1XJ9OREExlT",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "dJ2tPlVmpsJ0",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}